{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "495bfebf-1ee5-4e12-9644-87e0f74a3016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import itertools\n",
    "import pickle\n",
    "import copy\n",
    "import math\n",
    "import bisect\n",
    "import torch\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from itertools import combinations\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from model import BertClassifier\n",
    "\n",
    "\n",
    "F_P = {\n",
    "    0: [0,1],\n",
    "    1: [2,3],\n",
    "    2: [4,5,6,7,8,9,10,11],\n",
    "    3: [12,13],\n",
    "}\n",
    "\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, drop_rate=0.5):        \n",
    "        super(BertClassifier, self).__init__()\n",
    "        \n",
    "        self.transform_dim = nn.Sequential(\n",
    "            nn.Linear(input_dim, 768),\n",
    "        )\n",
    "        # bert-base-cased 768\n",
    "        # bert-large-cased 1024\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(512, output_dim),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(drop_rate),\n",
    "            # nn.Linear(128, 3)\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(drop_rate),\n",
    "            # nn.Linear(128, 64),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(drop_rate),\n",
    "            # nn.Linear(64, 32),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(drop_rate),\n",
    "            # nn.Linear(32, 3)\n",
    "            \n",
    "            )\n",
    "                        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        inputs = self.transform_dim(inputs) # (batch size, 10, 128)\n",
    "        outputs = self.bert(inputs_embeds=inputs) # (batch_size, sequence_length, hidden_size)\n",
    "        last_hidden_state = outputs.last_hidden_state # 32, 10, 768\n",
    "        outputs = last_hidden_state[:,-1,:]  # 32, 768    \n",
    "        outputs = self.classifier(outputs) # 32, 3\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "def generate_partition(f_part):  #input a dict\n",
    "    setnum = len(f_part)  #子集數\n",
    "    num_set = [i for i in range(setnum)]  #子集數list\n",
    "    cardlist = [len(i) for i in f_part.values()]  #各子集cardinality\n",
    "    f_list = [i for i in f_part.values()]  #list of list\n",
    "    all_subsets = []\n",
    "    for r in range(setnum+1):\n",
    "        for s in combinations(num_set, r):\n",
    "            tmp = []\n",
    "            for t in s:\n",
    "                tmp.extend(f_list[t])\n",
    "            all_subsets.append(np.array(sorted(tmp)))\n",
    "    # Create a hash table to store the index of each subset\n",
    "    subset_index_table = {}\n",
    "    for index, value in enumerate(all_subsets):\n",
    "        subset_index_table[tuple(value)] = index\n",
    "    return all_subsets, subset_index_table, len(all_subsets)\n",
    "\n",
    "\n",
    "class Hypercube_wpart:\n",
    "    '''\n",
    "    A class to create a hypercube object which stores values of vertices\n",
    "    '''    \n",
    "    #輸入維度\n",
    "    def __init__(self, partition):   #input a dict\n",
    "        self.f_part = partition\n",
    "        self.n_dim = len(self.f_part)\n",
    "        self.elem_count = sum([len(i) for i in self.f_part.values()])\n",
    "        # vertex_values is a dictionary to store the value of each vertex.\n",
    "        # Because np.array is not hashable, we use tuple to represent the vertex.\n",
    "        self.vertex_values = {}\n",
    "        self.vertices, self.vertex_index, self.vertex_num = generate_partition(self.f_part)  #vertex 上的value一次考慮整個subset\n",
    "        self.edges, self.edge_num = self.build_edges()\n",
    "        self.differential_matrix = None\n",
    "        self.weight_matrix = None\n",
    "        self.generate_min_l2_norm_matrix()\n",
    "    \n",
    "    def build_edges(self):\n",
    "        num_set = [i for i in range(self.n_dim)]  #子集數list\n",
    "        s_set = set(num_set)  #轉集合\n",
    "        cardlist = [len(i) for i in self.f_part.values()]  #各子集cardinality\n",
    "        f_list = [i for i in self.f_part.values()]  #list of list\n",
    "        #print(f'Receive {f_list}')\n",
    "        s_subset = set(tuple(i) for i in f_list)\n",
    "        edges = []\n",
    "        for r in range(self.n_dim): \n",
    "            for v in combinations(num_set, r):\n",
    "                v_set = set(v)\n",
    "                adjunct_v = s_set - v_set\n",
    "                for new_elem in adjunct_v:\n",
    "                    d_set = v_set | {new_elem}\n",
    "                    outlist, inlist = [],[]                    \n",
    "                    for k in v_set:\n",
    "                        outlist.extend(f_list[k])\n",
    "                    for l in d_set:\n",
    "                        inlist.extend(f_list[l])\n",
    "                    edges.append(((np.array(sorted(outlist))),np.array(sorted(inlist))))\n",
    "        return edges, len(edges)\n",
    "    \n",
    "    def get_elements(self, index):\n",
    "        return tuple(self.f_part[index])\n",
    "\n",
    "    def set_vertex_values(self, vertex_values):         #設置點值\n",
    "        for v in vertex_values:                         #用鍵值來做查找\n",
    "            self.vertex_values[v] = vertex_values[v]\n",
    "        \n",
    "    def does_edge_exist(self, v1, v2):\n",
    "        if abs(len(v1)-len(v2))==1:\n",
    "            interset = np.intersect1d(v1,v2)\n",
    "            smaller = v1 if len(v1)<len(v2) else v2\n",
    "            return True if np.array_equal(smaller, interset) else False\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    # Establish the matrix A in the above formula: AX-Y\n",
    "    def generate_differential_matrix(self):\n",
    "        if self.differential_matrix is None:\n",
    "            self.differential_matrix = np.zeros((self.edge_num+1, self.vertex_num))\n",
    "            for i,v_pair in enumerate(self.edges):\n",
    "                j = self.vertex_index[tuple(v_pair[1])]\n",
    "                k = self.vertex_index[tuple(v_pair[0])]\n",
    "                self.differential_matrix[i][j] = 1\n",
    "                self.differential_matrix[i][k] = -1\n",
    "            # Add one more equestion that x_0 = 0 into the matrix form\n",
    "            self.differential_matrix[-1][0]=1\n",
    "        return self.differential_matrix\n",
    "\n",
    "    # Pre-calcuate \"W=(A^T*A)^-1*A^T\" for the formula \"X = ((A^T*A)^-1*A^T)*Y\n",
    "    def generate_min_l2_norm_matrix(self):\n",
    "        matrix_A = self.generate_differential_matrix()\n",
    "        matrix_A_T = np.transpose(matrix_A)\n",
    "        self.weight_matrix = np.linalg.inv(matrix_A_T @ matrix_A) @ matrix_A_T\n",
    "\n",
    "    def get_gradient_vector(self):\n",
    "        gradient_vector = np.zeros(self.edge_num)\n",
    "        for i,v_pair in enumerate(self.edges):\n",
    "            gradient_vector[i] = self.vertex_values[tuple(v_pair[1])]-self.vertex_values[tuple(v_pair[0])]    \n",
    "        return gradient_vector      \n",
    "        \n",
    "    def get_partial_gradient_vector(self,subset_i):  #feature->subset\n",
    "        feature_i = self.get_elements(subset_i)\n",
    "        partial_gradient_vector = np.zeros(self.edge_num)\n",
    "        for i,v_pair in enumerate(self.edges):\n",
    "            if (not set(feature_i).issubset(set(v_pair[0]))) and (set(feature_i).issubset(set(v_pair[1]))):\n",
    "                partial_gradient_vector[i] = self.vertex_values[tuple(v_pair[1])]-self.vertex_values[tuple(v_pair[0])]    \n",
    "        return partial_gradient_vector\n",
    "    \n",
    "    def resolve_vi(self, subset_i, phi_0=0):  #feature->subset\n",
    "        pd = self.get_partial_gradient_vector(subset_i)\n",
    "        # Append equation x_0=0 at the end of partial gradient vector.\n",
    "        pd = np.append(pd, phi_0)\n",
    "        vi = self.weight_matrix @ pd\n",
    "        # Reconstruct the vertex values\n",
    "        new_vertices = {}\n",
    "        for i,v in enumerate(self.vertices):\n",
    "            new_vertices[tuple(v)] = vi[i]\n",
    "        return vi, new_vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9700d484-bdd5-45f7-9402-eaadd04bf64b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40976, 14) \n",
      "         0        1         2         3         4         5         6   \\\n",
      "0 -4.52677  0.05025 -1.046263 -0.543637 -0.597382 -1.525155 -0.339233   \n",
      "1 -4.52677  3.02762 -0.437004  0.823823 -0.261885 -1.495607 -0.834969   \n",
      "2 -4.52677  3.02762 -0.161470  0.451242  0.205512 -1.466059 -0.339233   \n",
      "3 -4.52677  3.02762 -0.574794  0.737841  0.888293 -1.515306 -0.339233   \n",
      "4 -4.52677  3.02762 -0.187324 -0.451514  2.848481 -1.517768  1.643708   \n",
      "\n",
      "         7         8         9         10         11        12        13  \n",
      "0 -0.423039  0.312691  0.434652 -0.189808 -10.071359 -0.636407 -1.197630  \n",
      "1  0.053782  0.421800  0.420173 -0.014344  -0.057674 -0.157584  1.399181  \n",
      "2 -0.416238  0.319511  0.246424  0.500908  -0.057674  0.142256 -0.945585  \n",
      "3 -0.421906  0.530908  0.485328 -0.267792  -0.057674  0.519077 -0.290205  \n",
      "4 -0.961784  0.449077  0.355017  0.360954  -0.057674  0.039486  2.032951  \n",
      "             0         1         2         3         4         5         6   \\\n",
      "6023  -0.296354  0.050250  0.625376  0.250624 -0.800528 -1.515306  2.139444   \n",
      "8192   0.761250  3.027620 -1.121511  0.680523  0.684116 -1.507919 -0.834969   \n",
      "36611 -1.001423  0.050250  0.768615  0.221970  0.628361  0.131999  1.643708   \n",
      "7854  -0.296354  0.050250 -0.200559  0.365270 -0.597504 -1.522693 -1.330704   \n",
      "31514 -1.001423 -1.367545  0.384313  0.508570 -0.497349  0.141848  0.652238   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "21805 -1.001423  0.050250 -0.271165 -0.935141 -0.899501  0.971657 -1.330704   \n",
      "35413  1.113784  0.050250  0.873504 -0.609143 -0.834490  0.136924 -0.339233   \n",
      "4032  -0.296354  0.050250 -0.588952 -1.185913 -0.462926 -1.512843 -1.330704   \n",
      "10551  0.761250  3.027620 -1.155655 -0.403750 -0.128411 -1.483295 -1.330704   \n",
      "23767  1.113784  0.050250  0.685139  0.193317 -0.777920  0.122150  1.147973   \n",
      "\n",
      "             7         8         9         10        11        12        13  \n",
      "6023  -1.012265  0.565005  0.536005  0.320086 -0.057674  0.345084 -0.276528  \n",
      "8192   0.051657  0.592282  0.485328  1.014444 -0.057674 -0.067252  0.692380  \n",
      "36611 -0.880407  0.503631  0.449131  1.096928 -0.057674 -0.167123  0.237131  \n",
      "7854   1.464693 -1.971767 -2.157093 -1.486293 -0.057674  0.519956 -0.233562  \n",
      "31514 -0.685298 -2.503671 -2.714536 -1.791731 -0.057674  1.426479  0.625776  \n",
      "...         ...       ...       ...       ...       ...       ...       ...  \n",
      "21805  2.325947 -2.026321 -2.215009 -1.524534 -0.057674 -0.435655 -1.111312  \n",
      "35413 -0.231744  0.496812  0.514286  0.214734 -0.057674 -0.083900 -1.173466  \n",
      "4032   1.468093 -1.937671 -2.186051 -1.330325 -0.057674  0.017755  0.325013  \n",
      "10551  1.478296 -2.026321 -2.186051 -1.583773 -0.057674  0.850074 -0.507895  \n",
      "23767 -0.799962  0.449077  0.405694  0.291094 -0.057674 -0.328995  0.219557  \n",
      "\n",
      "[1000 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "#sample n = 1000\n",
    "sample_num = 1000\n",
    "df = pd.read_csv('data_with_output.csv')\n",
    "df = df.drop(columns=['Output'])\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(df)\n",
    "nf_df = pd.DataFrame(X_standardized)\n",
    "print(nf_df.shape, '\\n', nf_df.head())\n",
    "s_df = nf_df.sample(n=sample_num, random_state=1000)\n",
    "print(s_df)\n",
    "s_df.to_csv(f'sample{sample_num}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11c2248a-3c55-4b27-9bed-3e1e3e5dfb6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 16) Empty DataFrame\n",
      "Columns: [(), (0, 1), (2, 3), (4, 5, 6, 7, 8, 9, 10, 11), (12, 13), (0, 1, 2, 3), (0, 1, 4, 5, 6, 7, 8, 9, 10, 11), (0, 1, 12, 13), (2, 3, 4, 5, 6, 7, 8, 9, 10, 11), (2, 3, 12, 13), (4, 5, 6, 7, 8, 9, 10, 11, 12, 13), (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11), (0, 1, 2, 3, 12, 13), (0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13), (2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13), (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13)]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# creat dataframe for saving vertex value\n",
    "c,_,_ = generate_partition(F_P)\n",
    "collist = [i.tolist() for i in c]\n",
    "colname = [tuple(i) for i in collist]\n",
    "vertex_df = pd.DataFrame(columns=colname)\n",
    "print(vertex_df.shape,vertex_df)\n",
    "if 'Output' in s_df.columns: \n",
    "    s_df = s_df.drop(columns=['Output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3767b812-78b6-4b2d-929f-8e6eb334767a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EQ': ['X_-TACT_TIME_mean', 'X_-CONVEYOR_SPEED_mean'], 'PUMP': ['PUMP_high', 'PUMP_low'], 'CH': ['CLN1_over-etching-ratio', 'CLN1_EPT_time', 'clean_count', 'EPT_clean_count_ratio', 'NH3_TREAT_-RF_FREQ-max', 'NH3_TREAT_-RF_FREQ-range', 'NH3_TREAT_-RF_FREQ-mean', 'NP_3_-MFC_VOL_SIH4-range'], 'VENT': ['VENT_high', 'VENT_low'], 'y': 'Output'}\n",
      "['X_-TACT_TIME_mean', 'X_-CONVEYOR_SPEED_mean', 'PUMP_high', 'PUMP_low', 'CLN1_over-etching-ratio', 'CLN1_EPT_time', 'clean_count', 'EPT_clean_count_ratio', 'NH3_TREAT_-RF_FREQ-max', 'NH3_TREAT_-RF_FREQ-range', 'NH3_TREAT_-RF_FREQ-mean', 'NP_3_-MFC_VOL_SIH4-range', 'VENT_high', 'VENT_low', 'Output']\n",
      "There are 4 sub processes. So the feature translation is [(0, 0), (0, 1), (1, 2), (1, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (2, 10), (2, 11), (3, 12), (3, 13)]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "s1_model_path = 'stage_1_checkpoint.pth'\n",
    "s1_model =  torch.load(s1_model_path, map_location=device).to(device)\n",
    "json_file_path = 'controllable_para_v014_14.json'\n",
    "tool_name = 'ASCVD'\n",
    "with open(json_file_path, 'r') as f:\n",
    "    params = json.load(f)[tool_name]\n",
    "    print(params)\n",
    "    f.close()\n",
    "target_features = []\n",
    "\n",
    "''' \n",
    "    The feature_translation is a list of tuples, each tuple contains two integers.\n",
    "    The tuples record the correponding position of the ith feature in the 4*freature matrix\n",
    "    as the input the prediction model.\n",
    "    Ex.\n",
    "    [(0, 0), (0, 1), \n",
    "     (1, 2), (1, 3), \n",
    "     (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (2, 10), (2, 11), \n",
    "     (3, 12),(3, 13)]\n",
    "'''\n",
    "# Flat the feature list and construct teh freature translation list to map feature into the input of the model\n",
    "feature_translation = []\n",
    "sub_op_num = 0\n",
    "for entry in [sub_op for _, sub_op in params.items()]:\n",
    "    if isinstance(entry, str):\n",
    "        target_features.append(entry)\n",
    "    else:\n",
    "        target_features.extend(entry)\n",
    "        feature_translation.extend([(sub_op_num,len(feature_translation)+j) for j in range(len(entry))])\n",
    "        sub_op_num = sub_op_num+1 \n",
    "\n",
    "print(target_features)\n",
    "print(f\"There are {sub_op_num} sub processes. So the feature translation is {feature_translation}\")\n",
    "\n",
    "def padding_zero(df, flag): \n",
    "    # 將一維參數matrix擴展為4維\n",
    "    data_arr = df.to_numpy()\n",
    "    result = []\n",
    "    for data in data_arr:\n",
    "        empty_arr = np.zeros((sub_op_num, len(feature_translation))) # chamber數 * 總參數數量\n",
    "        for i, pos in enumerate(feature_translation):\n",
    "            empty_arr[pos[0]][pos[1]] = data[i]\n",
    "        if(flag == 1): # bert.py使用\n",
    "            result.append(empty_arr)\n",
    "        if(flag == 2): # bert_du.py使用\n",
    "            result.append(empty_arr.tolist())\n",
    "    \n",
    "    if(flag == 1): # bert.py使用\n",
    "        result = pd.DataFrame({'X': [result[i] for i in range(len(result))]})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbfd56b8-2028-409e-b0b9-25a4bc4ec90a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0         1         2         3         4         5         6   \\\n",
      "0 -0.304923  0.005068  0.621896  0.260290 -0.944142 -1.535447  2.065472   \n",
      "1  0.742922  2.805651 -1.125247  0.730101  0.680975 -1.528042 -0.855299   \n",
      "2 -1.003487  0.005068  0.765156  0.228976  0.619946  0.115899  1.578677   \n",
      "3 -0.304923  0.005068 -0.204160  0.385580 -0.721909 -1.542852 -1.342094   \n",
      "4 -1.003487 -1.328544  0.380797  0.542183 -0.612278  0.125772  0.605086   \n",
      "\n",
      "         7         8         9         10        11        12        13  \n",
      "0 -0.999699  0.554105  0.528570  0.320436 -0.060108  0.351599 -0.355518  \n",
      "1  0.077931  0.581722  0.476775  1.071436 -0.060108 -0.111003  0.720120  \n",
      "2 -0.866143  0.491967  0.439779  1.160648 -0.060108 -0.223048  0.214724  \n",
      "3  1.509174 -2.014263 -2.223934 -1.633298 -0.060108  0.547788 -0.307819  \n",
      "4 -0.668519 -2.552792 -2.793673 -1.963652 -0.060108  1.564819  0.646180  \n"
     ]
    }
   ],
   "source": [
    "# scaler = StandardScaler()\n",
    "# X_standardized = scaler.fit_transform(s_df)\n",
    "# nf_df = pd.DataFrame(X_standardized)\n",
    "# print(nf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36b23c06-27b4-4fc1-b638-e18d05bc85dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0        1         2         3         4         5         6   \\\n",
      "0 -4.52677  0.05025 -1.046263 -0.543637 -0.597382 -1.525155 -0.339233   \n",
      "\n",
      "         7         8         9         10         11        12       13  \n",
      "0 -0.423039  0.312691  0.434652 -0.189808 -10.071359 -0.636407 -1.19763  \n",
      "[array([[ -4.52676957,   0.05025049,   0.        ,   0.        ,\n",
      "          0.        ,   0.        ,   0.        ,   0.        ,\n",
      "          0.        ,   0.        ,   0.        ,   0.        ,\n",
      "          0.        ,   0.        ],\n",
      "       [  0.        ,   0.        ,  -1.04626301,  -0.54363684,\n",
      "          0.        ,   0.        ,   0.        ,   0.        ,\n",
      "          0.        ,   0.        ,   0.        ,   0.        ,\n",
      "          0.        ,   0.        ],\n",
      "       [  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "         -0.59738229,  -1.52515499,  -0.3392332 ,  -0.42303916,\n",
      "          0.31269149,   0.43465178,  -0.18980823, -10.07135875,\n",
      "          0.        ,   0.        ],\n",
      "       [  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "          0.        ,   0.        ,   0.        ,   0.        ,\n",
      "          0.        ,   0.        ,   0.        ,   0.        ,\n",
      "         -0.63640726,  -1.19762975]])]\n",
      "(40976, 14)\n"
     ]
    }
   ],
   "source": [
    "# unit test of padding_zero function\n",
    "print(nf_df.head(1))\n",
    "result = padding_zero(nf_df, 1)\n",
    "print([e for e in result.iloc[0]])\n",
    "print(nf_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5999a090-307a-46bd-9fbb-f017f26e0de4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40976, 4, 14)\n"
     ]
    }
   ],
   "source": [
    "# Convert dataframe(n,1) in which element 'X' contains 4*feature array into nparray (n,4,14)\n",
    "nf_df_4d = padding_zero(nf_df,flag=1)\n",
    "nf_df_4d_arr = np.array([e for entry in nf_df_4d.values for e in entry])\n",
    "print(nf_df_4d_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "923421f3-5082-49e7-89e8-ef9189bd246a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m batch_data \u001b[38;5;241m=\u001b[39m batch_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 将数据传递给模型进行推理\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m batch_output \u001b[38;5;241m=\u001b[39m \u001b[43ms1_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m probs \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(batch_output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 将输出保存起来\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\folder\\pyfile\\shap_residual\\model.py:42\u001b[0m, in \u001b[0;36mBertClassifier.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m     41\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_dim(inputs) \u001b[38;5;66;03m# (batch size, 10, 128)\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (batch_size, sequence_length, hidden_size)\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     last_hidden_state \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state \u001b[38;5;66;03m# 32, 10, 768\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m last_hidden_state[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:]  \u001b[38;5;66;03m# 32, 768    \u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1012\u001b[0m )\n\u001b[1;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    598\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m         output_attentions,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:308\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    306\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([past_key_value[\u001b[38;5;241m1\u001b[39m], value_layer], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 308\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    309\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(hidden_states))\n\u001b[0;32m    311\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "s1_model.eval()\n",
    "nf_df_4d_tensor = torch.tensor(nf_df_4d_arr,dtype=torch.float)\n",
    "dataset = TensorDataset(nf_df_4d_tensor)\n",
    "batch_size = int(nf_df_4d_tensor.size()[0])\n",
    "loader = DataLoader(dataset, batch_size=batch_size)\n",
    "outputs = []\n",
    "with torch.no_grad():\n",
    "    for batch_data in loader:\n",
    "        # 将数据移到指定的设备上（如 CUDA 设备）\n",
    "        batch_data = batch_data[0].to(device)\n",
    "        \n",
    "        # 将数据传递给模型进行推理\n",
    "        batch_output = s1_model(batch_data)\n",
    "        probs = (torch.nn.functional.softmax(batch_output, dim=1))\n",
    "        # 将输出保存起来\n",
    "        outputs += probs\n",
    "print(len(outputs))\n",
    "#print(outputs)\n",
    "\n",
    "#取得模型平均和對應output\n",
    "# output_arr = np.array([output.cpu().numpy()[0] for output in outputs])\n",
    "# output_df = pd.DataFrame({'Output': output_arr})\n",
    "# new_df= pd.concat([feature_df,output_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0a07a5b-a3c0-4e4a-97b9-5c784fa57c54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_inference(data):  #standardize dataframe\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    s1_model.eval()\n",
    "    # scaler = StandardScaler()\n",
    "    if 'Output' in data.columns:\n",
    "        data = data.drop(columns=['Output'])\n",
    "    # data_standardized_df = pd.DataFrame(scaler.fit_transform(data))\n",
    "    #print(data.head())\n",
    "    #print(f'Standardized data:\\n{data_standardized_df.head(3)}')  # Debug\n",
    "    \n",
    "    data_4d = padding_zero(data,flag=1)\n",
    "    \n",
    "    #print(f'4D data:\\n{data_4d.head()}')  # Debug\n",
    "    \n",
    "    data_4d_array = np.array([e for entry in data_4d.values for e in entry])\n",
    "    data_4d_tensor = torch.tensor(data_4d_array,dtype=torch.float)\n",
    "    \n",
    "    #print(f'Tensor data shape: {data_4d_tensor.shape}')  # Debug \n",
    "    \n",
    "    my_dataset = TensorDataset(data_4d_tensor)\n",
    "    batch_size = min(256, int(data_4d_tensor.size()[0]))\n",
    "    my_loader = DataLoader(my_dataset, batch_size=batch_size,num_workers=4)\n",
    "    data_output = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data in my_loader:\n",
    "        # 将数据移到指定的设备上（如 CUDA 设备）\n",
    "            batch_data = batch_data[0].to(device)\n",
    "        # 将数据传递给模型进行推理\n",
    "            batch_output = s1_model(batch_data)\n",
    "            probs = (torch.nn.functional.softmax(batch_output, dim=1))\n",
    "        # 将输出保存起来\n",
    "            data_output += probs\n",
    "    data_output_array = np.array([output.cpu().numpy()[0] for output in data_output])\n",
    "    data_expectation_value = data_output_array.mean()\n",
    "    return data_expectation_value, data_output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a0ab578-5335-41f1-bd3f-0a68977c5ccc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base mean: 0.6264505982398987\n"
     ]
    }
   ],
   "source": [
    "# 過濾掉FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# First, calculate the output mean of the experiment data\n",
    "base_mean, details = model_inference(s_df)\n",
    "print(f'Base mean: {base_mean}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cb10aca-bad2-4171-87bf-80eaefe20f61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0         1         2         3         4         5         6   \\\n",
      "6023  -0.296354  0.050250  0.625376  0.250624 -0.800528 -1.515306  2.139444   \n",
      "8192   0.761250  3.027620 -1.121511  0.680523  0.684116 -1.507919 -0.834969   \n",
      "36611 -1.001423  0.050250  0.768615  0.221970  0.628361  0.131999  1.643708   \n",
      "7854  -0.296354  0.050250 -0.200559  0.365270 -0.597504 -1.522693 -1.330704   \n",
      "31514 -1.001423 -1.367545  0.384313  0.508570 -0.497349  0.141848  0.652238   \n",
      "\n",
      "             7         8         9         10        11        12        13  \n",
      "6023  -1.012265  0.565005  0.536005  0.320086 -0.057674  0.345084 -0.276528  \n",
      "8192   0.051657  0.592282  0.485328  1.014444 -0.057674 -0.067252  0.692380  \n",
      "36611 -0.880407  0.503631  0.449131  1.096928 -0.057674 -0.167123  0.237131  \n",
      "7854   1.464693 -1.971767 -2.157093 -1.486293 -0.057674  0.519956 -0.233562  \n",
      "31514 -0.685298 -2.503671 -2.714536 -1.791731 -0.057674  1.426479  0.625776  \n",
      "        0        1         2         3         4         5         6   \\\n",
      "0 -4.52677  0.05025 -1.046263 -0.543637 -0.597382 -1.525155 -0.339233   \n",
      "1 -4.52677  3.02762 -0.437004  0.823823 -0.261885 -1.495607 -0.834969   \n",
      "2 -4.52677  3.02762 -0.161470  0.451242  0.205512 -1.466059 -0.339233   \n",
      "3 -4.52677  3.02762 -0.574794  0.737841  0.888293 -1.515306 -0.339233   \n",
      "4 -4.52677  3.02762 -0.187324 -0.451514  2.848481 -1.517768  1.643708   \n",
      "\n",
      "         7         8         9         10         11        12        13  \n",
      "0 -0.423039  0.312691  0.434652 -0.189808 -10.071359 -0.636407 -1.197630  \n",
      "1  0.053782  0.421800  0.420173 -0.014344  -0.057674 -0.157584  1.399181  \n",
      "2 -0.416238  0.319511  0.246424  0.500908  -0.057674  0.142256 -0.945585  \n",
      "3 -0.421906  0.530908  0.485328 -0.267792  -0.057674  0.519077 -0.290205  \n",
      "4 -0.961784  0.449077  0.355017  0.360954  -0.057674  0.039486  2.032951  \n"
     ]
    }
   ],
   "source": [
    "para_num = 14\n",
    "subset_sum = 4\n",
    "#sample_num = 1000\n",
    "\n",
    "vertices_fl = 'E_vi_202406020109.pkl'\n",
    "# v_hist = vertices_fl.split('.')[0]+'_history.pkl'\n",
    "# try:\n",
    "#     with open(v_hist, 'rb') as f:\n",
    "#         print(f'Loading form {v_hist}...')\n",
    "#         v_history = pickle.load(f)\n",
    "#         print(type(v_history))\n",
    "# except FileNotFoundError:\n",
    "#     v_history = {}\n",
    "\n",
    "# print(f\"History vertices: {v_history}\")\n",
    "#mean_exp = new_df['Output'].mean()\n",
    "AUO_coalitions, _, _ = generate_partition(F_P) \n",
    "count_n = 0\n",
    "if 'Output' in df.columns: \n",
    "    df = df.drop(columns=['Output'])\n",
    "print(s_df.head())\n",
    "print(nf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c93146ab-8a35-42df-a02d-0b1d0b188149",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 10 vertex\n",
      "Saved 20 vertex\n",
      "Saved 30 vertex\n",
      "Saved 40 vertex\n",
      "Saved 50 vertex\n",
      "Saved 60 vertex\n",
      "Saved 70 vertex\n",
      "Saved 80 vertex\n",
      "Saved 90 vertex\n",
      "Saved 100 vertex\n",
      "Saved 110 vertex\n",
      "Saved 120 vertex\n",
      "Saved 130 vertex\n",
      "Saved 140 vertex\n",
      "Saved 150 vertex\n",
      "Saved 160 vertex\n",
      "Saved 170 vertex\n",
      "Saved 180 vertex\n",
      "Saved 190 vertex\n",
      "Saved 200 vertex\n",
      "Saved 210 vertex\n",
      "Saved 220 vertex\n",
      "Saved 230 vertex\n",
      "Saved 240 vertex\n",
      "Saved 250 vertex\n",
      "Saved 260 vertex\n",
      "Saved 270 vertex\n",
      "Saved 280 vertex\n",
      "Saved 290 vertex\n",
      "Saved 300 vertex\n",
      "Saved 310 vertex\n",
      "Saved 320 vertex\n",
      "Saved 330 vertex\n",
      "Saved 340 vertex\n",
      "Saved 350 vertex\n",
      "Saved 360 vertex\n",
      "Saved 370 vertex\n",
      "Saved 380 vertex\n",
      "Saved 390 vertex\n",
      "Saved 400 vertex\n",
      "Saved 410 vertex\n",
      "Saved 420 vertex\n",
      "Saved 430 vertex\n",
      "Saved 440 vertex\n",
      "Saved 450 vertex\n",
      "Saved 460 vertex\n",
      "Saved 470 vertex\n",
      "Saved 480 vertex\n",
      "Saved 490 vertex\n",
      "Saved 500 vertex\n",
      "Saved 510 vertex\n",
      "Saved 520 vertex\n",
      "Saved 530 vertex\n",
      "Saved 540 vertex\n",
      "Saved 550 vertex\n",
      "Saved 560 vertex\n",
      "Saved 570 vertex\n",
      "Saved 580 vertex\n",
      "Saved 590 vertex\n",
      "Saved 600 vertex\n",
      "Saved 610 vertex\n",
      "Saved 620 vertex\n",
      "Saved 630 vertex\n",
      "Saved 640 vertex\n",
      "Saved 650 vertex\n",
      "Saved 660 vertex\n",
      "Saved 670 vertex\n",
      "Saved 680 vertex\n",
      "Saved 690 vertex\n",
      "Saved 700 vertex\n",
      "Saved 710 vertex\n",
      "Saved 720 vertex\n",
      "Saved 730 vertex\n",
      "Saved 740 vertex\n",
      "Saved 750 vertex\n",
      "Saved 760 vertex\n",
      "Saved 770 vertex\n",
      "Saved 780 vertex\n",
      "Saved 790 vertex\n",
      "Saved 800 vertex\n",
      "Saved 810 vertex\n",
      "Saved 820 vertex\n",
      "Saved 830 vertex\n",
      "Saved 840 vertex\n",
      "Saved 850 vertex\n",
      "Saved 860 vertex\n",
      "Saved 870 vertex\n",
      "Saved 880 vertex\n",
      "Saved 890 vertex\n",
      "Saved 900 vertex\n",
      "Saved 910 vertex\n",
      "Saved 920 vertex\n",
      "Saved 930 vertex\n",
      "Saved 940 vertex\n",
      "Saved 950 vertex\n",
      "Saved 960 vertex\n",
      "Saved 970 vertex\n",
      "Saved 980 vertex\n",
      "Saved 990 vertex\n",
      "Saved 1000 vertex\n",
      "Saved 1010 vertex\n",
      "Saved 1020 vertex\n",
      "Saved 1030 vertex\n",
      "Saved 1040 vertex\n",
      "Saved 1050 vertex\n",
      "Saved 1060 vertex\n",
      "Saved 1070 vertex\n",
      "Saved 1080 vertex\n",
      "Saved 1090 vertex\n",
      "Saved 1100 vertex\n",
      "Saved 1110 vertex\n",
      "Saved 1120 vertex\n",
      "Saved 1130 vertex\n",
      "Saved 1140 vertex\n",
      "Saved 1150 vertex\n",
      "Saved 1160 vertex\n",
      "Saved 1170 vertex\n",
      "Saved 1180 vertex\n",
      "Saved 1190 vertex\n",
      "Saved 1200 vertex\n",
      "Saved 1210 vertex\n",
      "Saved 1220 vertex\n",
      "Saved 1230 vertex\n",
      "Saved 1240 vertex\n",
      "Saved 1250 vertex\n",
      "Saved 1260 vertex\n",
      "Saved 1270 vertex\n",
      "Saved 1280 vertex\n",
      "Saved 1290 vertex\n",
      "Saved 1300 vertex\n",
      "Saved 1310 vertex\n",
      "Saved 1320 vertex\n",
      "Saved 1330 vertex\n",
      "Saved 1340 vertex\n",
      "Saved 1350 vertex\n",
      "Saved 1360 vertex\n",
      "Saved 1370 vertex\n",
      "Saved 1380 vertex\n",
      "Saved 1390 vertex\n",
      "Saved 1400 vertex\n",
      "Saved 1410 vertex\n",
      "Saved 1420 vertex\n",
      "Saved 1430 vertex\n",
      "Saved 1440 vertex\n",
      "Saved 1450 vertex\n",
      "Saved 1460 vertex\n",
      "Saved 1470 vertex\n",
      "Saved 1480 vertex\n",
      "Saved 1490 vertex\n",
      "Saved 1500 vertex\n",
      "Saved 1510 vertex\n",
      "Saved 1520 vertex\n",
      "Saved 1530 vertex\n",
      "Saved 1540 vertex\n",
      "Saved 1550 vertex\n",
      "Saved 1560 vertex\n",
      "Saved 1570 vertex\n",
      "Saved 1580 vertex\n",
      "Saved 1590 vertex\n",
      "Saved 1600 vertex\n"
     ]
    }
   ],
   "source": [
    "for i_index in range(100):\n",
    "    #print(f'index = {i_index}')\n",
    "    instance = s_df.iloc[i_index]\n",
    "    coalition_estimated_values = {}\n",
    "    #print(f'coalition_estimated_values_start = {coalition_estimated_values}')\n",
    "    eval_df = nf_df.sample(n=100)  #要不斷sample\n",
    "    base_mean, details = model_inference(eval_df)\n",
    "    #print(f'Base mean: {base_mean}')\n",
    "    for coalition in AUO_coalitions: \n",
    "        vi_df = eval_df.copy()  #用copy()才不會去更改到原始的dataframe\n",
    "        if len(coalition)!=0:\n",
    "            vi_df.iloc[:,coalition] = instance.iloc[coalition]          \n",
    "        #print(f'coalition = {coalition}')\n",
    "        #print(vi_df.head(3))\n",
    "        exp, details = model_inference(vi_df)\n",
    "        #print(f'exp = {exp}')\n",
    "        coalition_estimated_values[tuple(coalition)] =  exp - base_mean\n",
    "        #print(f'coalition_estimated_values[tuple(coalition)] = {coalition_estimated_values[tuple(coalition)]}')\n",
    "        count_n += 1  \n",
    "        if count_n % 10 == 0:\n",
    "            with open(vertices_fl, 'wb') as f:\n",
    "                pickle.dump(vertex_df, f)\n",
    "            print(f\"Saved {count_n} vertex\")\n",
    "    #print(f'coalition_estimated_values = {coalition_estimated_values}')\n",
    "    instance_df = pd.DataFrame([coalition_estimated_values])\n",
    "    vertex_df = pd.concat([vertex_df, instance_df], ignore_index=True)            \n",
    "   \n",
    "\n",
    "with open(vertices_fl, 'wb') as f:\n",
    "    pickle.dump(vertex_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ece301d2-41f7-41d7-b856-23650edc11d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 16)\n"
     ]
    }
   ],
   "source": [
    "print(vertex_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73b9959d-e7c8-4656-a76d-5d28a8647dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1610 vertex\n",
      "Saved 1620 vertex\n",
      "Saved 1630 vertex\n",
      "Saved 1640 vertex\n",
      "Saved 1650 vertex\n",
      "Saved 1660 vertex\n",
      "Saved 1670 vertex\n",
      "Saved 1680 vertex\n",
      "Saved 1690 vertex\n",
      "Saved 1700 vertex\n",
      "Saved 1710 vertex\n",
      "Saved 1720 vertex\n",
      "Saved 1730 vertex\n",
      "Saved 1740 vertex\n",
      "Saved 1750 vertex\n",
      "Saved 1760 vertex\n",
      "Saved 1770 vertex\n",
      "Saved 1780 vertex\n",
      "Saved 1790 vertex\n",
      "Saved 1800 vertex\n",
      "Saved 1810 vertex\n",
      "Saved 1820 vertex\n",
      "Saved 1830 vertex\n",
      "Saved 1840 vertex\n",
      "Saved 1850 vertex\n",
      "Saved 1860 vertex\n",
      "Saved 1870 vertex\n",
      "Saved 1880 vertex\n",
      "Saved 1890 vertex\n",
      "Saved 1900 vertex\n",
      "Saved 1910 vertex\n",
      "Saved 1920 vertex\n",
      "Saved 1930 vertex\n",
      "Saved 1940 vertex\n",
      "Saved 1950 vertex\n",
      "Saved 1960 vertex\n",
      "Saved 1970 vertex\n",
      "Saved 1980 vertex\n",
      "Saved 1990 vertex\n",
      "Saved 2000 vertex\n",
      "Saved 2010 vertex\n",
      "Saved 2020 vertex\n",
      "Saved 2030 vertex\n",
      "Saved 2040 vertex\n",
      "Saved 2050 vertex\n",
      "Saved 2060 vertex\n",
      "Saved 2070 vertex\n",
      "Saved 2080 vertex\n",
      "Saved 2090 vertex\n",
      "Saved 2100 vertex\n",
      "Saved 2110 vertex\n",
      "Saved 2120 vertex\n",
      "Saved 2130 vertex\n",
      "Saved 2140 vertex\n",
      "Saved 2150 vertex\n",
      "Saved 2160 vertex\n",
      "Saved 2170 vertex\n",
      "Saved 2180 vertex\n",
      "Saved 2190 vertex\n",
      "Saved 2200 vertex\n",
      "Saved 2210 vertex\n",
      "Saved 2220 vertex\n",
      "Saved 2230 vertex\n",
      "Saved 2240 vertex\n",
      "Saved 2250 vertex\n",
      "Saved 2260 vertex\n",
      "Saved 2270 vertex\n",
      "Saved 2280 vertex\n",
      "Saved 2290 vertex\n",
      "Saved 2300 vertex\n",
      "Saved 2310 vertex\n",
      "Saved 2320 vertex\n",
      "Saved 2330 vertex\n",
      "Saved 2340 vertex\n",
      "Saved 2350 vertex\n",
      "Saved 2360 vertex\n",
      "Saved 2370 vertex\n",
      "Saved 2380 vertex\n",
      "Saved 2390 vertex\n",
      "Saved 2400 vertex\n",
      "Saved 2410 vertex\n",
      "Saved 2420 vertex\n",
      "Saved 2430 vertex\n",
      "Saved 2440 vertex\n",
      "Saved 2450 vertex\n",
      "Saved 2460 vertex\n",
      "Saved 2470 vertex\n",
      "Saved 2480 vertex\n",
      "Saved 2490 vertex\n",
      "Saved 2500 vertex\n",
      "Saved 2510 vertex\n",
      "Saved 2520 vertex\n",
      "Saved 2530 vertex\n",
      "Saved 2540 vertex\n",
      "Saved 2550 vertex\n",
      "Saved 2560 vertex\n",
      "Saved 2570 vertex\n",
      "Saved 2580 vertex\n",
      "Saved 2590 vertex\n",
      "Saved 2600 vertex\n",
      "Saved 2610 vertex\n",
      "Saved 2620 vertex\n",
      "Saved 2630 vertex\n",
      "Saved 2640 vertex\n",
      "Saved 2650 vertex\n",
      "Saved 2660 vertex\n",
      "Saved 2670 vertex\n",
      "Saved 2680 vertex\n",
      "Saved 2690 vertex\n",
      "Saved 2700 vertex\n",
      "Saved 2710 vertex\n",
      "Saved 2720 vertex\n",
      "Saved 2730 vertex\n",
      "Saved 2740 vertex\n",
      "Saved 2750 vertex\n",
      "Saved 2760 vertex\n",
      "Saved 2770 vertex\n",
      "Saved 2780 vertex\n",
      "Saved 2790 vertex\n",
      "Saved 2800 vertex\n",
      "Saved 2810 vertex\n",
      "Saved 2820 vertex\n",
      "Saved 2830 vertex\n",
      "Saved 2840 vertex\n",
      "Saved 2850 vertex\n",
      "Saved 2860 vertex\n",
      "Saved 2870 vertex\n",
      "Saved 2880 vertex\n",
      "Saved 2890 vertex\n",
      "Saved 2900 vertex\n",
      "Saved 2910 vertex\n",
      "Saved 2920 vertex\n",
      "Saved 2930 vertex\n",
      "Saved 2940 vertex\n",
      "Saved 2950 vertex\n",
      "Saved 2960 vertex\n",
      "Saved 2970 vertex\n",
      "Saved 2980 vertex\n",
      "Saved 2990 vertex\n",
      "Saved 3000 vertex\n",
      "Saved 3010 vertex\n",
      "Saved 3020 vertex\n",
      "Saved 3030 vertex\n",
      "Saved 3040 vertex\n",
      "Saved 3050 vertex\n",
      "Saved 3060 vertex\n",
      "Saved 3070 vertex\n",
      "Saved 3080 vertex\n",
      "Saved 3090 vertex\n",
      "Saved 3100 vertex\n",
      "Saved 3110 vertex\n",
      "Saved 3120 vertex\n",
      "Saved 3130 vertex\n",
      "Saved 3140 vertex\n",
      "Saved 3150 vertex\n",
      "Saved 3160 vertex\n",
      "Saved 3170 vertex\n",
      "Saved 3180 vertex\n",
      "Saved 3190 vertex\n",
      "Saved 3200 vertex\n"
     ]
    }
   ],
   "source": [
    "for i_index in range(100,200):\n",
    "    #print(f'index = {i_index}')\n",
    "    instance = s_df.iloc[i_index]\n",
    "    coalition_estimated_values = {}\n",
    "    #print(f'coalition_estimated_values_start = {coalition_estimated_values}')\n",
    "    eval_df = nf_df.sample(n=100)  #要不斷sample\n",
    "    base_mean, details = model_inference(eval_df)\n",
    "    #print(f'Base mean: {base_mean}')\n",
    "    for coalition in AUO_coalitions: \n",
    "        vi_df = eval_df.copy()  #用copy()才不會去更改到原始的dataframe\n",
    "        if len(coalition)!=0:\n",
    "            vi_df.iloc[:,coalition] = instance.iloc[coalition]          \n",
    "        #print(f'coalition = {coalition}')\n",
    "        #print(vi_df.head(3))\n",
    "        exp, details = model_inference(vi_df)\n",
    "        #print(f'exp = {exp}')\n",
    "        coalition_estimated_values[tuple(coalition)] =  exp - base_mean\n",
    "        #print(f'coalition_estimated_values[tuple(coalition)] = {coalition_estimated_values[tuple(coalition)]}')\n",
    "        count_n += 1  \n",
    "        if count_n % 10 == 0:\n",
    "            with open(vertices_fl, 'wb') as f:\n",
    "                pickle.dump(vertex_df, f)\n",
    "            print(f\"Saved {count_n} vertex\")\n",
    "    #print(f'coalition_estimated_values = {coalition_estimated_values}')\n",
    "    instance_df = pd.DataFrame([coalition_estimated_values])\n",
    "    vertex_df = pd.concat([vertex_df, instance_df], ignore_index=True)            \n",
    "   \n",
    "\n",
    "with open(vertices_fl, 'wb') as f:\n",
    "    pickle.dump(vertex_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6adbb665-70d7-481a-979a-29f58eea48ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 16)\n",
      "    ()    (0, 1)    (2, 3)  (4, 5, 6, 7, 8, 9, 10, 11)  (12, 13)  \\\n",
      "0  0.0 -0.056516 -0.018734                    0.045142 -0.007646   \n",
      "1  0.0 -0.001230 -0.003270                   -0.000125 -0.006132   \n",
      "2  0.0  0.119478 -0.026200                   -0.011343  0.005535   \n",
      "3  0.0 -0.078831  0.004747                   -0.341013 -0.012067   \n",
      "4  0.0  0.079112 -0.027634                   -0.260073 -0.014424   \n",
      "\n",
      "   (0, 1, 2, 3)  (0, 1, 4, 5, 6, 7, 8, 9, 10, 11)  (0, 1, 12, 13)  \\\n",
      "0     -0.106934                          0.020485       -0.078097   \n",
      "1      0.054099                         -0.063368       -0.026565   \n",
      "2      0.123736                          0.221338        0.119354   \n",
      "3     -0.081727                         -0.353102       -0.099172   \n",
      "4      0.059395                          0.129242        0.109980   \n",
      "\n",
      "   (2, 3, 4, 5, 6, 7, 8, 9, 10, 11)  (2, 3, 12, 13)  \\\n",
      "0                          0.013593       -0.028639   \n",
      "1                          0.031311       -0.008005   \n",
      "2                         -0.011873       -0.022852   \n",
      "3                         -0.355631       -0.002952   \n",
      "4                         -0.297029       -0.022607   \n",
      "\n",
      "   (4, 5, 6, 7, 8, 9, 10, 11, 12, 13)  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11)  \\\n",
      "0                            0.044837                               -0.041033   \n",
      "1                           -0.022733                                0.087804   \n",
      "2                           -0.002370                                0.278268   \n",
      "3                           -0.358193                               -0.365948   \n",
      "4                           -0.234586                                0.254836   \n",
      "\n",
      "   (0, 1, 2, 3, 12, 13)  (0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13)  \\\n",
      "0             -0.122005                                  0.016068   \n",
      "1              0.043252                                 -0.107559   \n",
      "2              0.124045                                  0.216966   \n",
      "3             -0.106712                                 -0.372335   \n",
      "4              0.120659                                  0.289370   \n",
      "\n",
      "   (2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13)  \\\n",
      "0                                  0.020176   \n",
      "1                                 -0.005122   \n",
      "2                                 -0.013015   \n",
      "3                                 -0.369411   \n",
      "4                                 -0.290483   \n",
      "\n",
      "   (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13)  \n",
      "0                                       -0.056160  \n",
      "1                                        0.048381  \n",
      "2                                        0.281505  \n",
      "3                                       -0.383749  \n",
      "4                                        0.316869  \n"
     ]
    }
   ],
   "source": [
    "print(vertex_df.shape)\n",
    "print(vertex_df.iloc[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "594c3eff-68df-4608-ba5a-822028e059d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3210 vertex\n",
      "Saved 3220 vertex\n",
      "Saved 3230 vertex\n",
      "Saved 3240 vertex\n",
      "Saved 3250 vertex\n",
      "Saved 3260 vertex\n",
      "Saved 3270 vertex\n",
      "Saved 3280 vertex\n",
      "Saved 3290 vertex\n",
      "Saved 3300 vertex\n",
      "Saved 3310 vertex\n",
      "Saved 3320 vertex\n",
      "Saved 3330 vertex\n",
      "Saved 3340 vertex\n",
      "Saved 3350 vertex\n",
      "Saved 3360 vertex\n",
      "Saved 3370 vertex\n",
      "Saved 3380 vertex\n",
      "Saved 3390 vertex\n",
      "Saved 3400 vertex\n",
      "Saved 3410 vertex\n",
      "Saved 3420 vertex\n",
      "Saved 3430 vertex\n",
      "Saved 3440 vertex\n",
      "Saved 3450 vertex\n",
      "Saved 3460 vertex\n",
      "Saved 3470 vertex\n",
      "Saved 3480 vertex\n",
      "Saved 3490 vertex\n",
      "Saved 3500 vertex\n",
      "Saved 3510 vertex\n",
      "Saved 3520 vertex\n",
      "Saved 3530 vertex\n",
      "Saved 3540 vertex\n",
      "Saved 3550 vertex\n",
      "Saved 3560 vertex\n",
      "Saved 3570 vertex\n",
      "Saved 3580 vertex\n",
      "Saved 3590 vertex\n",
      "Saved 3600 vertex\n",
      "Saved 3610 vertex\n",
      "Saved 3620 vertex\n",
      "Saved 3630 vertex\n",
      "Saved 3640 vertex\n",
      "Saved 3650 vertex\n",
      "Saved 3660 vertex\n",
      "Saved 3670 vertex\n",
      "Saved 3680 vertex\n",
      "Saved 3690 vertex\n",
      "Saved 3700 vertex\n",
      "Saved 3710 vertex\n",
      "Saved 3720 vertex\n",
      "Saved 3730 vertex\n",
      "Saved 3740 vertex\n",
      "Saved 3750 vertex\n",
      "Saved 3760 vertex\n",
      "Saved 3770 vertex\n",
      "Saved 3780 vertex\n",
      "Saved 3790 vertex\n",
      "Saved 3800 vertex\n",
      "Saved 3810 vertex\n",
      "Saved 3820 vertex\n",
      "Saved 3830 vertex\n",
      "Saved 3840 vertex\n",
      "Saved 3850 vertex\n",
      "Saved 3860 vertex\n",
      "Saved 3870 vertex\n",
      "Saved 3880 vertex\n",
      "Saved 3890 vertex\n",
      "Saved 3900 vertex\n",
      "Saved 3910 vertex\n",
      "Saved 3920 vertex\n",
      "Saved 3930 vertex\n",
      "Saved 3940 vertex\n",
      "Saved 3950 vertex\n",
      "Saved 3960 vertex\n",
      "Saved 3970 vertex\n",
      "Saved 3980 vertex\n",
      "Saved 3990 vertex\n",
      "Saved 4000 vertex\n",
      "Saved 4010 vertex\n",
      "Saved 4020 vertex\n",
      "Saved 4030 vertex\n",
      "Saved 4040 vertex\n",
      "Saved 4050 vertex\n",
      "Saved 4060 vertex\n",
      "Saved 4070 vertex\n",
      "Saved 4080 vertex\n",
      "Saved 4090 vertex\n",
      "Saved 4100 vertex\n",
      "Saved 4110 vertex\n",
      "Saved 4120 vertex\n",
      "Saved 4130 vertex\n",
      "Saved 4140 vertex\n",
      "Saved 4150 vertex\n",
      "Saved 4160 vertex\n",
      "Saved 4170 vertex\n",
      "Saved 4180 vertex\n",
      "Saved 4190 vertex\n",
      "Saved 4200 vertex\n",
      "Saved 4210 vertex\n",
      "Saved 4220 vertex\n",
      "Saved 4230 vertex\n",
      "Saved 4240 vertex\n",
      "Saved 4250 vertex\n",
      "Saved 4260 vertex\n",
      "Saved 4270 vertex\n",
      "Saved 4280 vertex\n",
      "Saved 4290 vertex\n",
      "Saved 4300 vertex\n",
      "Saved 4310 vertex\n",
      "Saved 4320 vertex\n",
      "Saved 4330 vertex\n",
      "Saved 4340 vertex\n",
      "Saved 4350 vertex\n",
      "Saved 4360 vertex\n",
      "Saved 4370 vertex\n",
      "Saved 4380 vertex\n",
      "Saved 4390 vertex\n",
      "Saved 4400 vertex\n",
      "Saved 4410 vertex\n",
      "Saved 4420 vertex\n",
      "Saved 4430 vertex\n",
      "Saved 4440 vertex\n",
      "Saved 4450 vertex\n",
      "Saved 4460 vertex\n",
      "Saved 4470 vertex\n",
      "Saved 4480 vertex\n",
      "Saved 4490 vertex\n",
      "Saved 4500 vertex\n",
      "Saved 4510 vertex\n",
      "Saved 4520 vertex\n",
      "Saved 4530 vertex\n",
      "Saved 4540 vertex\n",
      "Saved 4550 vertex\n",
      "Saved 4560 vertex\n",
      "Saved 4570 vertex\n",
      "Saved 4580 vertex\n",
      "Saved 4590 vertex\n",
      "Saved 4600 vertex\n",
      "Saved 4610 vertex\n",
      "Saved 4620 vertex\n",
      "Saved 4630 vertex\n",
      "Saved 4640 vertex\n",
      "Saved 4650 vertex\n",
      "Saved 4660 vertex\n",
      "Saved 4670 vertex\n",
      "Saved 4680 vertex\n",
      "Saved 4690 vertex\n",
      "Saved 4700 vertex\n",
      "Saved 4710 vertex\n",
      "Saved 4720 vertex\n",
      "Saved 4730 vertex\n",
      "Saved 4740 vertex\n",
      "Saved 4750 vertex\n",
      "Saved 4760 vertex\n",
      "Saved 4770 vertex\n",
      "Saved 4780 vertex\n",
      "Saved 4790 vertex\n",
      "Saved 4800 vertex\n",
      "Saved 4810 vertex\n",
      "Saved 4820 vertex\n",
      "Saved 4830 vertex\n",
      "Saved 4840 vertex\n",
      "Saved 4850 vertex\n",
      "Saved 4860 vertex\n",
      "Saved 4870 vertex\n",
      "Saved 4880 vertex\n",
      "Saved 4890 vertex\n",
      "Saved 4900 vertex\n",
      "Saved 4910 vertex\n",
      "Saved 4920 vertex\n",
      "Saved 4930 vertex\n",
      "Saved 4940 vertex\n",
      "Saved 4950 vertex\n",
      "Saved 4960 vertex\n",
      "Saved 4970 vertex\n",
      "Saved 4980 vertex\n",
      "Saved 4990 vertex\n",
      "Saved 5000 vertex\n",
      "Saved 5010 vertex\n",
      "Saved 5020 vertex\n",
      "Saved 5030 vertex\n",
      "Saved 5040 vertex\n",
      "Saved 5050 vertex\n",
      "Saved 5060 vertex\n",
      "Saved 5070 vertex\n",
      "Saved 5080 vertex\n",
      "Saved 5090 vertex\n",
      "Saved 5100 vertex\n",
      "Saved 5110 vertex\n",
      "Saved 5120 vertex\n",
      "Saved 5130 vertex\n",
      "Saved 5140 vertex\n",
      "Saved 5150 vertex\n",
      "Saved 5160 vertex\n",
      "Saved 5170 vertex\n",
      "Saved 5180 vertex\n",
      "Saved 5190 vertex\n",
      "Saved 5200 vertex\n",
      "Saved 5210 vertex\n",
      "Saved 5220 vertex\n",
      "Saved 5230 vertex\n",
      "Saved 5240 vertex\n",
      "Saved 5250 vertex\n",
      "Saved 5260 vertex\n",
      "Saved 5270 vertex\n",
      "Saved 5280 vertex\n",
      "Saved 5290 vertex\n",
      "Saved 5300 vertex\n",
      "Saved 5310 vertex\n",
      "Saved 5320 vertex\n",
      "Saved 5330 vertex\n",
      "Saved 5340 vertex\n",
      "Saved 5350 vertex\n",
      "Saved 5360 vertex\n",
      "Saved 5370 vertex\n",
      "Saved 5380 vertex\n",
      "Saved 5390 vertex\n",
      "Saved 5400 vertex\n",
      "Saved 5410 vertex\n",
      "Saved 5420 vertex\n",
      "Saved 5430 vertex\n",
      "Saved 5440 vertex\n",
      "Saved 5450 vertex\n",
      "Saved 5460 vertex\n",
      "Saved 5470 vertex\n",
      "Saved 5480 vertex\n",
      "Saved 5490 vertex\n",
      "Saved 5500 vertex\n",
      "Saved 5510 vertex\n",
      "Saved 5520 vertex\n",
      "Saved 5530 vertex\n",
      "Saved 5540 vertex\n",
      "Saved 5550 vertex\n",
      "Saved 5560 vertex\n",
      "Saved 5570 vertex\n",
      "Saved 5580 vertex\n",
      "Saved 5590 vertex\n",
      "Saved 5600 vertex\n",
      "Saved 5610 vertex\n",
      "Saved 5620 vertex\n",
      "Saved 5630 vertex\n",
      "Saved 5640 vertex\n",
      "Saved 5650 vertex\n",
      "Saved 5660 vertex\n",
      "Saved 5670 vertex\n",
      "Saved 5680 vertex\n",
      "Saved 5690 vertex\n",
      "Saved 5700 vertex\n",
      "Saved 5710 vertex\n",
      "Saved 5720 vertex\n",
      "Saved 5730 vertex\n",
      "Saved 5740 vertex\n",
      "Saved 5750 vertex\n",
      "Saved 5760 vertex\n",
      "Saved 5770 vertex\n",
      "Saved 5780 vertex\n",
      "Saved 5790 vertex\n",
      "Saved 5800 vertex\n",
      "Saved 5810 vertex\n",
      "Saved 5820 vertex\n",
      "Saved 5830 vertex\n",
      "Saved 5840 vertex\n",
      "Saved 5850 vertex\n",
      "Saved 5860 vertex\n",
      "Saved 5870 vertex\n",
      "Saved 5880 vertex\n",
      "Saved 5890 vertex\n",
      "Saved 5900 vertex\n",
      "Saved 5910 vertex\n",
      "Saved 5920 vertex\n",
      "Saved 5930 vertex\n",
      "Saved 5940 vertex\n",
      "Saved 5950 vertex\n",
      "Saved 5960 vertex\n",
      "Saved 5970 vertex\n",
      "Saved 5980 vertex\n",
      "Saved 5990 vertex\n",
      "Saved 6000 vertex\n",
      "Saved 6010 vertex\n",
      "Saved 6020 vertex\n",
      "Saved 6030 vertex\n",
      "Saved 6040 vertex\n",
      "Saved 6050 vertex\n",
      "Saved 6060 vertex\n",
      "Saved 6070 vertex\n",
      "Saved 6080 vertex\n",
      "Saved 6090 vertex\n",
      "Saved 6100 vertex\n",
      "Saved 6110 vertex\n",
      "Saved 6120 vertex\n",
      "Saved 6130 vertex\n",
      "Saved 6140 vertex\n",
      "Saved 6150 vertex\n",
      "Saved 6160 vertex\n",
      "Saved 6170 vertex\n",
      "Saved 6180 vertex\n",
      "Saved 6190 vertex\n",
      "Saved 6200 vertex\n",
      "Saved 6210 vertex\n",
      "Saved 6220 vertex\n",
      "Saved 6230 vertex\n",
      "Saved 6240 vertex\n",
      "Saved 6250 vertex\n",
      "Saved 6260 vertex\n",
      "Saved 6270 vertex\n",
      "Saved 6280 vertex\n",
      "Saved 6290 vertex\n",
      "Saved 6300 vertex\n",
      "Saved 6310 vertex\n",
      "Saved 6320 vertex\n",
      "Saved 6330 vertex\n",
      "Saved 6340 vertex\n",
      "Saved 6350 vertex\n",
      "Saved 6360 vertex\n",
      "Saved 6370 vertex\n",
      "Saved 6380 vertex\n",
      "Saved 6390 vertex\n",
      "Saved 6400 vertex\n"
     ]
    }
   ],
   "source": [
    "for i_index in range(200,400):\n",
    "    #print(f'index = {i_index}')\n",
    "    instance = s_df.iloc[i_index]\n",
    "    coalition_estimated_values = {}\n",
    "    #print(f'coalition_estimated_values_start = {coalition_estimated_values}')\n",
    "    eval_df = nf_df.sample(n=100)  #要不斷sample\n",
    "    base_mean, details = model_inference(eval_df)\n",
    "    #print(f'Base mean: {base_mean}')\n",
    "    for coalition in AUO_coalitions: \n",
    "        vi_df = eval_df.copy()  #用copy()才不會去更改到原始的dataframe\n",
    "        if len(coalition)!=0:\n",
    "            vi_df.iloc[:,coalition] = instance.iloc[coalition]          \n",
    "        #print(f'coalition = {coalition}')\n",
    "        #print(vi_df.head(3))\n",
    "        exp, details = model_inference(vi_df)\n",
    "        #print(f'exp = {exp}')\n",
    "        coalition_estimated_values[tuple(coalition)] =  exp - base_mean\n",
    "        #print(f'coalition_estimated_values[tuple(coalition)] = {coalition_estimated_values[tuple(coalition)]}')\n",
    "        count_n += 1  \n",
    "        if count_n % 10 == 0:\n",
    "            with open(vertices_fl, 'wb') as f:\n",
    "                pickle.dump(vertex_df, f)\n",
    "            print(f\"Saved {count_n} vertex\")\n",
    "    #print(f'coalition_estimated_values = {coalition_estimated_values}')\n",
    "    instance_df = pd.DataFrame([coalition_estimated_values])\n",
    "    vertex_df = pd.concat([vertex_df, instance_df], ignore_index=True)            \n",
    "   \n",
    "\n",
    "with open(vertices_fl, 'wb') as f:\n",
    "    pickle.dump(vertex_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b3edde7-9227-476f-8160-3fb9a39fab2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.tail of       ()    (0, 1)    (2, 3)  (4, 5, 6, 7, 8, 9, 10, 11)  (12, 13)  \\\n",
      "0    0.0 -0.056516 -0.018734                    0.045142 -0.007646   \n",
      "1    0.0 -0.001230 -0.003270                   -0.000125 -0.006132   \n",
      "2    0.0  0.119478 -0.026200                   -0.011343  0.005535   \n",
      "3    0.0 -0.078831  0.004747                   -0.341013 -0.012067   \n",
      "4    0.0  0.079112 -0.027634                   -0.260073 -0.014424   \n",
      "..   ...       ...       ...                         ...       ...   \n",
      "395  0.0 -0.064708 -0.005147                    0.101029 -0.007977   \n",
      "396  0.0  0.055786 -0.020408                    0.059892 -0.034912   \n",
      "397  0.0 -0.026820 -0.023915                    0.016681  0.003001   \n",
      "398  0.0 -0.077867  0.017598                    0.081263 -0.020323   \n",
      "399  0.0 -0.073594 -0.019889                    0.108478  0.007847   \n",
      "\n",
      "     (0, 1, 2, 3)  (0, 1, 4, 5, 6, 7, 8, 9, 10, 11)  (0, 1, 12, 13)  \\\n",
      "0       -0.106934                          0.020485       -0.078097   \n",
      "1        0.054099                         -0.063368       -0.026565   \n",
      "2        0.123736                          0.221338        0.119354   \n",
      "3       -0.081727                         -0.353102       -0.099172   \n",
      "4        0.059395                          0.129242        0.109980   \n",
      "..            ...                               ...             ...   \n",
      "395     -0.057073                          0.046742       -0.065423   \n",
      "396      0.039505                          0.213909        0.051529   \n",
      "397     -0.042432                         -0.050602       -0.024431   \n",
      "398     -0.018893                          0.095185       -0.080381   \n",
      "399     -0.101365                          0.056089       -0.072601   \n",
      "\n",
      "     (2, 3, 4, 5, 6, 7, 8, 9, 10, 11)  (2, 3, 12, 13)  \\\n",
      "0                            0.013593       -0.028639   \n",
      "1                            0.031311       -0.008005   \n",
      "2                           -0.011873       -0.022852   \n",
      "3                           -0.355631       -0.002952   \n",
      "4                           -0.297029       -0.022607   \n",
      "..                                ...             ...   \n",
      "395                          0.098155       -0.015533   \n",
      "396                          0.019022       -0.039541   \n",
      "397                         -0.025403       -0.026466   \n",
      "398                          0.092022       -0.002172   \n",
      "399                          0.043153       -0.027565   \n",
      "\n",
      "     (4, 5, 6, 7, 8, 9, 10, 11, 12, 13)  \\\n",
      "0                              0.044837   \n",
      "1                             -0.022733   \n",
      "2                             -0.002370   \n",
      "3                             -0.358193   \n",
      "4                             -0.234586   \n",
      "..                                  ...   \n",
      "395                            0.093622   \n",
      "396                            0.008446   \n",
      "397                            0.014264   \n",
      "398                            0.081798   \n",
      "399                            0.135247   \n",
      "\n",
      "     (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11)  (0, 1, 2, 3, 12, 13)  \\\n",
      "0                                 -0.041033             -0.122005   \n",
      "1                                  0.087804              0.043252   \n",
      "2                                  0.278268              0.124045   \n",
      "3                                 -0.365948             -0.106712   \n",
      "4                                  0.254836              0.120659   \n",
      "..                                      ...                   ...   \n",
      "395                                0.068119             -0.049490   \n",
      "396                                0.174544              0.054279   \n",
      "397                               -0.066832             -0.042579   \n",
      "398                                0.114047             -0.035205   \n",
      "399                               -0.087717             -0.124392   \n",
      "\n",
      "     (0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13)  \\\n",
      "0                                    0.016068   \n",
      "1                                   -0.107559   \n",
      "2                                    0.216966   \n",
      "3                                   -0.372335   \n",
      "4                                    0.289370   \n",
      "..                                        ...   \n",
      "395                                  0.041662   \n",
      "396                                  0.198978   \n",
      "397                                 -0.046067   \n",
      "398                                  0.088054   \n",
      "399                                  0.092630   \n",
      "\n",
      "     (2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13)  \\\n",
      "0                                    0.020176   \n",
      "1                                   -0.005122   \n",
      "2                                   -0.013015   \n",
      "3                                   -0.369411   \n",
      "4                                   -0.290483   \n",
      "..                                        ...   \n",
      "395                                  0.088593   \n",
      "396                                 -0.030960   \n",
      "397                                 -0.033290   \n",
      "398                                  0.083259   \n",
      "399                                  0.045762   \n",
      "\n",
      "     (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13)  \n",
      "0                                         -0.056160  \n",
      "1                                          0.048381  \n",
      "2                                          0.281505  \n",
      "3                                         -0.383749  \n",
      "4                                          0.316869  \n",
      "..                                              ...  \n",
      "395                                        0.051872  \n",
      "396                                        0.165712  \n",
      "397                                       -0.071983  \n",
      "398                                        0.094759  \n",
      "399                                       -0.107150  \n",
      "\n",
      "[400 rows x 16 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(vertex_df.tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d78529c9-f0e2-4817-816e-12971f2d96c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vertex_df.to_csv('vertex_val.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "2c047693-4ed3-4235-a7a5-8f2bfb4e2ea0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X_-TACT_TIME_mean          165.0\n",
       "X_-CONVEYOR_SPEED_mean    4200.0\n",
       "Name: 8192, dtype: float64"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a5a966e5-56c0-4344-a214-04e29fc1fabf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "a3918687-28fa-4f18-ac07-0cf8ab203ce6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_-TACT_TIME_mean</th>\n",
       "      <th>X_-CONVEYOR_SPEED_mean</th>\n",
       "      <th>PUMP_high</th>\n",
       "      <th>PUMP_low</th>\n",
       "      <th>CLN1_over-etching-ratio</th>\n",
       "      <th>CLN1_EPT_time</th>\n",
       "      <th>clean_count</th>\n",
       "      <th>EPT_clean_count_ratio</th>\n",
       "      <th>NH3_TREAT_-RF_FREQ-max</th>\n",
       "      <th>NH3_TREAT_-RF_FREQ-range</th>\n",
       "      <th>NH3_TREAT_-RF_FREQ-mean</th>\n",
       "      <th>NP_3_-MFC_VOL_SIH4-range</th>\n",
       "      <th>VENT_high</th>\n",
       "      <th>VENT_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>165</td>\n",
       "      <td>4200</td>\n",
       "      <td>28942.83333</td>\n",
       "      <td>12625.12</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>10008</td>\n",
       "      <td>2</td>\n",
       "      <td>5004.0</td>\n",
       "      <td>13988</td>\n",
       "      <td>451</td>\n",
       "      <td>13675.7692</td>\n",
       "      <td>1</td>\n",
       "      <td>16471.57333</td>\n",
       "      <td>7963.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16943</th>\n",
       "      <td>165</td>\n",
       "      <td>4200</td>\n",
       "      <td>28942.83333</td>\n",
       "      <td>12625.12</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>10008</td>\n",
       "      <td>2</td>\n",
       "      <td>5004.0</td>\n",
       "      <td>13988</td>\n",
       "      <td>451</td>\n",
       "      <td>13675.7692</td>\n",
       "      <td>1</td>\n",
       "      <td>16471.57333</td>\n",
       "      <td>7963.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39741</th>\n",
       "      <td>165</td>\n",
       "      <td>4200</td>\n",
       "      <td>28942.83333</td>\n",
       "      <td>12625.12</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>10008</td>\n",
       "      <td>2</td>\n",
       "      <td>5004.0</td>\n",
       "      <td>13988</td>\n",
       "      <td>451</td>\n",
       "      <td>13675.7692</td>\n",
       "      <td>1</td>\n",
       "      <td>16471.57333</td>\n",
       "      <td>7963.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18373</th>\n",
       "      <td>165</td>\n",
       "      <td>4200</td>\n",
       "      <td>28942.83333</td>\n",
       "      <td>12625.12</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>10008</td>\n",
       "      <td>2</td>\n",
       "      <td>5004.0</td>\n",
       "      <td>13988</td>\n",
       "      <td>451</td>\n",
       "      <td>13675.7692</td>\n",
       "      <td>1</td>\n",
       "      <td>16471.57333</td>\n",
       "      <td>7963.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38402</th>\n",
       "      <td>165</td>\n",
       "      <td>4200</td>\n",
       "      <td>28942.83333</td>\n",
       "      <td>12625.12</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>10008</td>\n",
       "      <td>2</td>\n",
       "      <td>5004.0</td>\n",
       "      <td>13988</td>\n",
       "      <td>451</td>\n",
       "      <td>13675.7692</td>\n",
       "      <td>1</td>\n",
       "      <td>16471.57333</td>\n",
       "      <td>7963.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13747</th>\n",
       "      <td>165</td>\n",
       "      <td>4200</td>\n",
       "      <td>28942.83333</td>\n",
       "      <td>12625.12</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>10008</td>\n",
       "      <td>2</td>\n",
       "      <td>5004.0</td>\n",
       "      <td>13988</td>\n",
       "      <td>451</td>\n",
       "      <td>13675.7692</td>\n",
       "      <td>1</td>\n",
       "      <td>16471.57333</td>\n",
       "      <td>7963.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40005</th>\n",
       "      <td>165</td>\n",
       "      <td>4200</td>\n",
       "      <td>28942.83333</td>\n",
       "      <td>12625.12</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>10008</td>\n",
       "      <td>2</td>\n",
       "      <td>5004.0</td>\n",
       "      <td>13988</td>\n",
       "      <td>451</td>\n",
       "      <td>13675.7692</td>\n",
       "      <td>1</td>\n",
       "      <td>16471.57333</td>\n",
       "      <td>7963.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22449</th>\n",
       "      <td>165</td>\n",
       "      <td>4200</td>\n",
       "      <td>28942.83333</td>\n",
       "      <td>12625.12</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>10008</td>\n",
       "      <td>2</td>\n",
       "      <td>5004.0</td>\n",
       "      <td>13988</td>\n",
       "      <td>451</td>\n",
       "      <td>13675.7692</td>\n",
       "      <td>1</td>\n",
       "      <td>16471.57333</td>\n",
       "      <td>7963.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17346</th>\n",
       "      <td>165</td>\n",
       "      <td>4200</td>\n",
       "      <td>28942.83333</td>\n",
       "      <td>12625.12</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>10008</td>\n",
       "      <td>2</td>\n",
       "      <td>5004.0</td>\n",
       "      <td>13988</td>\n",
       "      <td>451</td>\n",
       "      <td>13675.7692</td>\n",
       "      <td>1</td>\n",
       "      <td>16471.57333</td>\n",
       "      <td>7963.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32202</th>\n",
       "      <td>165</td>\n",
       "      <td>4200</td>\n",
       "      <td>28942.83333</td>\n",
       "      <td>12625.12</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>10008</td>\n",
       "      <td>2</td>\n",
       "      <td>5004.0</td>\n",
       "      <td>13988</td>\n",
       "      <td>451</td>\n",
       "      <td>13675.7692</td>\n",
       "      <td>1</td>\n",
       "      <td>16471.57333</td>\n",
       "      <td>7963.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       X_-TACT_TIME_mean  X_-CONVEYOR_SPEED_mean    PUMP_high  PUMP_low  \\\n",
       "14635                165                    4200  28942.83333  12625.12   \n",
       "16943                165                    4200  28942.83333  12625.12   \n",
       "39741                165                    4200  28942.83333  12625.12   \n",
       "18373                165                    4200  28942.83333  12625.12   \n",
       "38402                165                    4200  28942.83333  12625.12   \n",
       "...                  ...                     ...          ...       ...   \n",
       "13747                165                    4200  28942.83333  12625.12   \n",
       "40005                165                    4200  28942.83333  12625.12   \n",
       "22449                165                    4200  28942.83333  12625.12   \n",
       "17346                165                    4200  28942.83333  12625.12   \n",
       "32202                165                    4200  28942.83333  12625.12   \n",
       "\n",
       "       CLN1_over-etching-ratio  CLN1_EPT_time  clean_count  \\\n",
       "14635                 0.007494          10008            2   \n",
       "16943                 0.007494          10008            2   \n",
       "39741                 0.007494          10008            2   \n",
       "18373                 0.007494          10008            2   \n",
       "38402                 0.007494          10008            2   \n",
       "...                        ...            ...          ...   \n",
       "13747                 0.007494          10008            2   \n",
       "40005                 0.007494          10008            2   \n",
       "22449                 0.007494          10008            2   \n",
       "17346                 0.007494          10008            2   \n",
       "32202                 0.007494          10008            2   \n",
       "\n",
       "       EPT_clean_count_ratio  NH3_TREAT_-RF_FREQ-max  \\\n",
       "14635                 5004.0                   13988   \n",
       "16943                 5004.0                   13988   \n",
       "39741                 5004.0                   13988   \n",
       "18373                 5004.0                   13988   \n",
       "38402                 5004.0                   13988   \n",
       "...                      ...                     ...   \n",
       "13747                 5004.0                   13988   \n",
       "40005                 5004.0                   13988   \n",
       "22449                 5004.0                   13988   \n",
       "17346                 5004.0                   13988   \n",
       "32202                 5004.0                   13988   \n",
       "\n",
       "       NH3_TREAT_-RF_FREQ-range  NH3_TREAT_-RF_FREQ-mean  \\\n",
       "14635                       451               13675.7692   \n",
       "16943                       451               13675.7692   \n",
       "39741                       451               13675.7692   \n",
       "18373                       451               13675.7692   \n",
       "38402                       451               13675.7692   \n",
       "...                         ...                      ...   \n",
       "13747                       451               13675.7692   \n",
       "40005                       451               13675.7692   \n",
       "22449                       451               13675.7692   \n",
       "17346                       451               13675.7692   \n",
       "32202                       451               13675.7692   \n",
       "\n",
       "       NP_3_-MFC_VOL_SIH4-range    VENT_high  VENT_low  \n",
       "14635                         1  16471.57333   7963.95  \n",
       "16943                         1  16471.57333   7963.95  \n",
       "39741                         1  16471.57333   7963.95  \n",
       "18373                         1  16471.57333   7963.95  \n",
       "38402                         1  16471.57333   7963.95  \n",
       "...                         ...          ...       ...  \n",
       "13747                         1  16471.57333   7963.95  \n",
       "40005                         1  16471.57333   7963.95  \n",
       "22449                         1  16471.57333   7963.95  \n",
       "17346                         1  16471.57333   7963.95  \n",
       "32202                         1  16471.57333   7963.95  \n",
       "\n",
       "[1000 rows x 14 columns]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "a1ee154d-ed42-4171-8a33-86d0bdcff554",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ()    (0, 1)    (2, 3)  (4, 5, 6, 7, 8, 9, 10, 11)  (12, 13)  \\\n",
      "0   0.000000 -0.066063 -0.016101                   -0.006981 -0.001281   \n",
      "1   0.000000 -0.066063 -0.016101                   -0.006981 -0.001281   \n",
      "2   0.000000 -0.066063 -0.016101                   -0.006981 -0.001281   \n",
      "3   0.000000 -0.066063 -0.016101                   -0.006981 -0.001281   \n",
      "4   0.000000 -0.066063 -0.016101                   -0.006981 -0.001281   \n",
      "5   0.000000 -0.066063 -0.016101                   -0.006981 -0.001281   \n",
      "6   0.000000 -0.066063 -0.016101                   -0.006981 -0.001281   \n",
      "7   0.000000 -0.066063 -0.016101                   -0.006981 -0.001281   \n",
      "8   0.000000 -0.066063 -0.016101                   -0.006981 -0.001281   \n",
      "9   0.000000 -0.066063 -0.016101                   -0.006981 -0.001281   \n",
      "10 -0.004485 -0.068757 -0.017998                    0.002006 -0.001748   \n",
      "11 -0.004485 -0.068757 -0.017998                    0.002006 -0.001748   \n",
      "12  0.000000 -0.056675 -0.012028                   -0.006897 -0.011382   \n",
      "13  0.000000 -0.064380 -0.003855                   -0.015234  0.000598   \n",
      "\n",
      "    (0, 1, 2, 3)  (0, 1, 4, 5, 6, 7, 8, 9, 10, 11)  (0, 1, 12, 13)  \\\n",
      "0      -0.090180                         -0.115100       -0.068755   \n",
      "1      -0.090180                         -0.115100       -0.068755   \n",
      "2      -0.090180                         -0.115100       -0.068755   \n",
      "3      -0.090180                         -0.115100       -0.068755   \n",
      "4      -0.090180                         -0.115100       -0.068755   \n",
      "5      -0.090180                         -0.115100       -0.068755   \n",
      "6      -0.090180                         -0.115100       -0.068755   \n",
      "7      -0.090180                         -0.115100       -0.068755   \n",
      "8      -0.090180                         -0.115100       -0.068755   \n",
      "9      -0.090180                         -0.115100       -0.068755   \n",
      "10     -0.088826                         -0.109127       -0.063173   \n",
      "11     -0.088826                         -0.109127       -0.063173   \n",
      "12     -0.089291                         -0.099113       -0.064284   \n",
      "13     -0.087272                         -0.108781       -0.065471   \n",
      "\n",
      "    (2, 3, 4, 5, 6, 7, 8, 9, 10, 11)  (2, 3, 12, 13)  \\\n",
      "0                          -0.040076       -0.019120   \n",
      "1                          -0.040076       -0.019120   \n",
      "2                          -0.040076       -0.019120   \n",
      "3                          -0.040076       -0.019120   \n",
      "4                          -0.040076       -0.019120   \n",
      "5                          -0.040076       -0.019120   \n",
      "6                          -0.040076       -0.019120   \n",
      "7                          -0.040076       -0.019120   \n",
      "8                          -0.040076       -0.019120   \n",
      "9                          -0.040076       -0.019120   \n",
      "10                         -0.038730       -0.016325   \n",
      "11                         -0.038730       -0.016325   \n",
      "12                         -0.059584       -0.026393   \n",
      "13                         -0.048407       -0.008197   \n",
      "\n",
      "    (4, 5, 6, 7, 8, 9, 10, 11, 12, 13)  \\\n",
      "0                            -0.018878   \n",
      "1                            -0.018878   \n",
      "2                            -0.018878   \n",
      "3                            -0.018878   \n",
      "4                            -0.018878   \n",
      "5                            -0.018878   \n",
      "6                            -0.018878   \n",
      "7                            -0.018878   \n",
      "8                            -0.018878   \n",
      "9                            -0.018878   \n",
      "10                           -0.011344   \n",
      "11                           -0.011344   \n",
      "12                           -0.028916   \n",
      "13                           -0.024940   \n",
      "\n",
      "    (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11)  (0, 1, 2, 3, 12, 13)  \\\n",
      "0                                -0.165973             -0.092637   \n",
      "1                                -0.165973             -0.092637   \n",
      "2                                -0.165973             -0.092637   \n",
      "3                                -0.165973             -0.092637   \n",
      "4                                -0.165973             -0.092637   \n",
      "5                                -0.165973             -0.092637   \n",
      "6                                -0.165973             -0.092637   \n",
      "7                                -0.165973             -0.092637   \n",
      "8                                -0.165973             -0.092637   \n",
      "9                                -0.165973             -0.092637   \n",
      "10                               -0.165092             -0.085324   \n",
      "11                               -0.165092             -0.085324   \n",
      "12                               -0.165636             -0.091920   \n",
      "13                               -0.158334             -0.088593   \n",
      "\n",
      "    (0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13)  \\\n",
      "0                                  -0.133291   \n",
      "1                                  -0.133291   \n",
      "2                                  -0.133291   \n",
      "3                                  -0.133291   \n",
      "4                                  -0.133291   \n",
      "5                                  -0.133291   \n",
      "6                                  -0.133291   \n",
      "7                                  -0.133291   \n",
      "8                                  -0.133291   \n",
      "9                                  -0.133291   \n",
      "10                                 -0.130644   \n",
      "11                                 -0.130644   \n",
      "12                                 -0.127603   \n",
      "13                                 -0.125254   \n",
      "\n",
      "    (2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13)  \\\n",
      "0                                  -0.049671   \n",
      "1                                  -0.049671   \n",
      "2                                  -0.049671   \n",
      "3                                  -0.049671   \n",
      "4                                  -0.049671   \n",
      "5                                  -0.049671   \n",
      "6                                  -0.049671   \n",
      "7                                  -0.049671   \n",
      "8                                  -0.049671   \n",
      "9                                  -0.049671   \n",
      "10                                 -0.048855   \n",
      "11                                 -0.048855   \n",
      "12                                 -0.073206   \n",
      "13                                 -0.054615   \n",
      "\n",
      "    (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13)  \n",
      "0                                        -0.179834  \n",
      "1                                        -0.179834  \n",
      "2                                        -0.179834  \n",
      "3                                        -0.179834  \n",
      "4                                        -0.179834  \n",
      "5                                        -0.179834  \n",
      "6                                        -0.179834  \n",
      "7                                        -0.179834  \n",
      "8                                        -0.179834  \n",
      "9                                        -0.179834  \n",
      "10                                       -0.179834  \n",
      "11                                       -0.179834  \n",
      "12                                       -0.182172  \n",
      "13                                       -0.171123  \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162271b1-d383-42a2-a9bb-5f502d787f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
